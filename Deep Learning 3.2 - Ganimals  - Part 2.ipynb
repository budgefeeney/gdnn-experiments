{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2814cc0",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "This notebook approaches the uses a toy dataset, the [Google Quick Draw! dataset](https://github.com/googlecreativelab/quickdraw-dataset#preprocessed-dataset) to peform initial experiments iwth Generative Adversarial Network learning. The book calls these \"Ganimals\" but this has a specific meaning elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17413a9",
   "metadata": {
    "id": "yZNrOfPKKRBD"
   },
   "source": [
    "# Prelude\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74f3cf3",
   "metadata": {
    "id": "urYi27pPKpYt"
   },
   "outputs": [],
   "source": [
    "from typing import NamedTuple, List, Union, Tuple, Optional, Dict\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "import sys\n",
    "import math\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Flatten, Dense, Input, Conv2D, Conv2DTranspose, \\\n",
    "  Activation, LeakyReLU, Dropout, BatchNormalization, Reshape, Lambda, UpSampling2D\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "from tensorflow.keras import backend as K  # tensor operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b346e021",
   "metadata": {
    "id": "aOtAWo9DaQVL"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.INFO, datefmt='%I:%M:%S')\n",
    "logging.info(\"Logging set up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4f5fe1",
   "metadata": {
    "id": "RSZrj3tuKsE-"
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6f6554",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNNING_ON_COLAB = False\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    RUNNING_ON_COLAB = TRUE\n",
    "except:\n",
    "    logging.info(\"Running locally. Not running on Google Colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d783cf",
   "metadata": {
    "id": "iVMjwo2_JOXU"
   },
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = pathlib.Path.cwd()  / 'checkpoints' / 'celeb_faces'\n",
    "\n",
    "if not CHECKPOINT_DIR.exists():\n",
    "  CHECKPOINT_DIR.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5431b6",
   "metadata": {
    "id": "eaIFQfVMKY2y"
   },
   "outputs": [],
   "source": [
    "USER_FIRST_NAME: Optional[str] = \"bryan\"\n",
    "\n",
    "if RUNNING_ON_COLAB:\n",
    "    GDRIVE_DIR_BASE = pathlib.Path('/') / 'content' / 'drive'\n",
    "    MY_GDRIVE_DIR = GDRIVE_DIR_BASE / 'MyDrive'\n",
    "else:\n",
    "    # Assume we're running my laptop in Windows\n",
    "    MY_GDRIVE_DIR = pathlib.Path.home() / 'Google Drive'\n",
    "    if not MY_GDRIVE_DIR.exists():\n",
    "        # Assume we're running my laptop in Linux, with GDrive on the Windows partition\n",
    "        ROOT_DIR = pathlib.Path(pathlib.Path.home().root)\n",
    "        WIN_DRIVE = ROOT_DIR / 'media' / pathlib.Path.home().name / 'Blade 15'\n",
    "        WIN_HOME = WIN_DRIVE / 'Users' / pathlib.Path.home().name\n",
    "        if not WIN_HOME.exists():\n",
    "            import os\n",
    "            import pwd\n",
    "            first_name = USER_FIRST_NAME or pwd.getpwuid(os.getuid())[4].lower().split(' ')[0]\n",
    "            WIN_HOME = WIN_DRIVE / 'Users' / first_name\n",
    "\n",
    "        MY_GDRIVE_DIR = WIN_HOME / 'Google Drive'\n",
    "        \n",
    "COLAB_DATA_DIR = MY_GDRIVE_DIR / 'Colab Data'\n",
    "CAMEL_NPY_FILE = COLAB_DATA_DIR / 'Google Quick Draw' / 'full_numpy_bitmap_camel.npy' \n",
    "assert CAMEL_NPY_FILE.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa05d21",
   "metadata": {
    "id": "Ybnng82vaRq6"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig()\n",
    "level = logging.INFO\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(level)\n",
    "for handler in logger.handlers:\n",
    "    handler.setLevel(level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927971e5",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df68ac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUICK_DRAW_SHAPE = (28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8a0b89",
   "metadata": {},
   "source": [
    "## Check GPU Enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555fcdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow v2.4 on CUDA 11\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)  \n",
    "\n",
    "# Can also write\n",
    "# import tensorflow as tf\n",
    "# physical_devices = tf.config.list_physical_devices('GPU') \n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84039c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "local_devices = device_lib.list_local_devices()\n",
    "print(local_devices)\n",
    "\n",
    "assert \"GPU\" in [d.device_type for d in local_devices], \"No GPU Enabled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322d7565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "491c3e6c",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d281a3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "camels = np.load(CAMEL_NPY_FILE)\n",
    "CAMEL_NPY_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e672e470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_camels(camels: np.ndarray) -> None:\n",
    "    figscale = 8\n",
    "    ncols = 8\n",
    "    \n",
    "    if camels.shape[0] > 64:\n",
    "        figscale /= 4\n",
    "        ncols *= 4\n",
    "        \n",
    "    nrows = min(32, max(1, int(camels.shape[0] / ncols)))\n",
    "\n",
    "    fig, axes = plt.subplots(figsize=(figscale*ncols, figscale*nrows), dpi=120, nrows=nrows, ncols=ncols)\n",
    "\n",
    "    row = -1\n",
    "    for i in range(nrows * ncols):\n",
    "        row = i // ncols\n",
    "        col = i % ncols\n",
    "        ax = axes[row, col]\n",
    "\n",
    "        ax.imshow(camels[i].reshape(QUICK_DRAW_SHAPE), cmap='Greys')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    \n",
    "plot_camels(camels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689645c7",
   "metadata": {},
   "source": [
    "So learning to draw a camel from this dataset would be an astonishing feat. For one thing, they're facing different directions: most left, but some right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f773bd9",
   "metadata": {},
   "source": [
    "# Network Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a367dcc",
   "metadata": {},
   "source": [
    "So a GAN consists of two competing components\n",
    "\n",
    "1. A discriminator which, given a bunch of pixels, states whether it is a real image or a fake. It's trained (at least partly?!) on real image\n",
    "2. A generator, which generates pixels that confuse the discriminator.\n",
    "    \n",
    "In a way it's sort of a reinforcement-learning setup, where you learn the... value function?\n",
    "\n",
    "There are two ways of expanding out from a latent to the true dimension.\n",
    "\n",
    "1. Previously we looked at Conv2DTranspose, which inserts zero values inbetween strides\n",
    "    1. An issue with this is that even when results are good, you can see a checkerboard pattern in the find detail.\n",
    "2. However an alternative is to Upsample2D (which just repeats values to fill in the gaps) and then use a _standard_ convolution\n",
    "    1. How the fuck _this_ works is open for debate but *shrug*\n",
    "    \n",
    " Things not mentioned in the book\n",
    " \n",
    "  1. The LeakyReLU units have `alpha=0.2`. The default is 0.3. What is `alpha` is described below\n",
    "  2. The weight init is N(0, 0.2), and not one of the more complex Glorot ones.\n",
    "  \n",
    " LeakyReLU definition is\n",
    " \n",
    " $$\n",
    " \\begin{aligned}\n",
    " f(x) & = \\left\\{  \\begin{array}{lr}\n",
    "     \\alpha \\cdot x & x < 0 \\\\\n",
    "     x & x \\geq 0\n",
    " \\end{array}\\right.\n",
    " &\n",
    " \\implies f'(x) & = \\left\\{  \\begin{array}{lr}\n",
    "     \\alpha & x < 0 \\\\\n",
    "     1 & x \\geq 0\n",
    " \\end{array}\\right.\n",
    " \\end{aligned}\n",
    " $$\n",
    " \n",
    " Essentially $\\alpha$ is the negative slope coefficient. sUually, one chooses $0 < \\alpha < 1$. The special case of $\\alpha=0$ is an ordinary ReLU, and the special case of $\\alpha=1$ is just the identity function. Choosing $\\alpha > 1$ implies that the composition of many such layers might exhibit exploding gradients, which is undesirable. Also, choosing $\\alpha<0$ makes $f$ a non-monotonic function shaped something like a V. Non-monotonic functions have recently become more popular (e.g. mish and swish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbab4804",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GanBuilder(NamedTuple):\n",
    "    input_dim: Tuple[int, int, int]\n",
    "    \n",
    "    disc_conv_filter_count: List[int]\n",
    "    disc_conv_kernel_size: List[int]\n",
    "    disc_conv_strides: List[int]\n",
    "    disc_batch_norm_momentum: Optional[float]\n",
    "    disc_activation: str\n",
    "    disc_dropout_rate: Optional[float]\n",
    "    disc_learning_rate: float\n",
    "    \n",
    "    gen_initial_dense_layer_size: Tuple[int, int, int]\n",
    "    gen_upsample: List[int]\n",
    "        \n",
    "    gen_conv_filter_count: List[int]\n",
    "    gen_conv_kernel_size: List[int]\n",
    "    gen_conv_strides: List[int]\n",
    "    gen_batch_norm_momentum: Optional[float]\n",
    "    gen_activation: str\n",
    "    gen_dropout_rate: Optional[float]\n",
    "    gen_learning_rate: float\n",
    "        \n",
    "    optimiser: str\n",
    "    \n",
    "    z_dim: int\n",
    "        \n",
    "    weight_init: str = 'glorot_uniform'  # they use keras.initializers.RandomNormal(mean=0, stddev=0.02)\n",
    "        \n",
    "    models: Dict[str, Optional[Model]] = {'d': None, 'g': None, 'c': None}\n",
    "        \n",
    "    disc_loss_real: List[float] = []\n",
    "    disc_loss_fake: List[float] = []\n",
    "    disc_acc_real: List[float] = []\n",
    "    disc_acc_fake: List[float] = []\n",
    "    gen_loss: List[float] = []\n",
    "    gen_acc: List[float] = []\n",
    "        \n",
    "    epochs_completed_wrapper: List[int] = [ 0 ]  # Hack around immutability of namedtuple\n",
    "        \n",
    "    \n",
    "    @property\n",
    "    def epochs_completed(self) -> int:\n",
    "        return self.epochs_completed_wrapper[0]\n",
    "    \n",
    "    @epochs_completed.setter\n",
    "    def epochs_completed(self, val: int) -> None:\n",
    "        self.epochs_completed_wrapper[0] = val\n",
    "        \n",
    "    @property\n",
    "    def disc_loss_avg(self) -> List[float]:\n",
    "        return list(0.5 * np.array(self.disc_loss_real) + np.array(self.disc_loss_fake))\n",
    "    \n",
    "    @property\n",
    "    def disc_acc_avg(self) -> List[float]:\n",
    "        return list(0.5 * np.array(self.disc_acc_real) + np.array(self.disc_acc_fake))\n",
    "        \n",
    "    @property\n",
    "    def discriminator_model(self) -> Model:\n",
    "        if not self.models.get('d', None):\n",
    "            self.models['d'] = self._build_discriminator()\n",
    "        return self.models['d']\n",
    "        \n",
    "    @property\n",
    "    def generator_model(self) -> Model:\n",
    "        if not self.models.get('g', None):\n",
    "            self.models['g'] = self._build_generator()\n",
    "        return self.models['g']\n",
    "        \n",
    "    @property\n",
    "    def compiled_model(self) -> Model:\n",
    "        \"\"\"\n",
    "        Returns a trainable no-op model, that takes a latent representation\n",
    "        of an image as the input, generates an image from it, and feeds it\n",
    "        to the discriminator, finally outputing a real / fake classification.\n",
    "        \n",
    "        This permutes the discriminator model: it disables training.\n",
    "        \"\"\"\n",
    "        assert self.disc_learning_rate > self.gen_learning_rate, \\\n",
    "            \"Want a 'stronger' discriminator than generator, so prefer to make larger, faster steps to convergence\"\n",
    "        \n",
    "        if not self.models.get('m', None):\n",
    "            # Compile the discriminator model, and freeze it\n",
    "            self.discriminator_model.compile(\n",
    "                optimizer=RMSprop(lr=self.disc_learning_rate),\n",
    "                loss='binary_crossentropy',  # it's a classification problem\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "\n",
    "            # Use the discriminator model to create and input / output pair\n",
    "            # and then use it and that pair to build a paired model\n",
    "            self.discriminator_model.trainable = False\n",
    "            \n",
    "            model_input = Input(shape=(self.z_dim, ), name='model_input')\n",
    "            model_output = self.discriminator_model(\n",
    "                self.generator_model(\n",
    "                    model_input\n",
    "                )\n",
    "            )\n",
    "            model = Model(model_input, model_output)\n",
    "            \n",
    "            model.compile(\n",
    "                optimizer=RMSprop(lr=self.gen_learning_rate),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "            \n",
    "            self.models['m'] = model\n",
    "            self.discriminator_model.trainable = True\n",
    "            \n",
    "        return self.models['m']\n",
    "            \n",
    "    \n",
    "    def train(self, x_train: np.ndarray, epoch_count: int = 2000, batch_size: int = 64) -> Tuple[Model, Model]:\n",
    "        \"\"\"\n",
    "        Since this is a GAN, we have two models to train in an alternating fashion\n",
    "        \n",
    "        First we train the discriminator on a binary classification problem using real images\n",
    "        (labelled 1) and fake images (labelled 2)\n",
    "        \n",
    "        Then we fix the discriminator, and train a paired model of generator and discriminator,\n",
    "        on a binary classification of low-dimensional representations, all of which are labelled\n",
    "        1 (real). Since only the generator parameters are trainable, the solution to this \n",
    "        optimisation problem is to make the generator generate the most plausible images according\n",
    "        to the discriminator.\n",
    "        \"\"\"\n",
    "        # TODO xtrain should be Union[np.ndarray, ImageDataGenerator] in which either xtrain=xtrain[idx]\n",
    "        # or xtrain=next(xtrain) with an assert on batch_sizes\n",
    "        \n",
    "        _ = self.compiled_model  # Make sure the models are all compiled\n",
    "        \n",
    "        for epoch in range(self.epochs_completed, self.epochs_completed + epoch_count):\n",
    "            self.discriminator_model.trainable = True\n",
    "            for _ in range(10):\n",
    "                self._train_discriminator(x_train, batch_size)\n",
    "            \n",
    "            self.discriminator_model.trainable = False\n",
    "            self._train_generator(batch_size)\n",
    "            self.log_last_epoch_stats_to_info(epoch)\n",
    "            self.epochs_completed += 1\n",
    "            \n",
    "        \n",
    "    def log_last_epoch_stats_to_info(self, epoch) -> None:\n",
    "        msg = f\"{epoch:04d} [D loss: ({self.disc_loss_avg[-1]:.3f})(R {self.disc_loss_real[-1]:.3f}, F {self.disc_loss_fake[-1]:.3f})] \" \\\n",
    "              f\"[D acc: ({self.disc_acc_avg[-1]:.3f})(R {self.disc_acc_real[-1]:.3f}, F {self.disc_acc_fake[-1]:.3f})] \" \\\n",
    "              f\"[G loss: {self.gen_loss[-1]:.3f}] [G acc: {self.gen_acc[-1]:.3f}]\"\n",
    "        \n",
    "        logging.info(msg)\n",
    "        \n",
    "        \n",
    "    def _low_dim_image_sample(self, batch_size: int) -> np.ndarray:\n",
    "        # FIXME Generator doesn't adhere to stdev of real generator... in fact using a std normal seems dodge as fuck...,\n",
    "        #       Shouldn't it be MV Beta?\n",
    "        fake_image_low_dim = rd.normal(0, 1, (batch_size, self.z_dim))\n",
    "        return fake_image_low_dim\n",
    "    \n",
    "        \n",
    "    def _train_discriminator(self, x_train: np.ndarray, batch_size: int) -> None:\n",
    "        \"\"\"\n",
    "        This trains just the discriminator on its own, on a simple binary classification problem,\n",
    "        where real images are labelled as 1, and images from the generator are labelled as 0.\n",
    "        \"\"\"\n",
    "        assert self.discriminator_model.trainable, \"Discriminator model is not trainable\"\n",
    "        assert all(l.trainable for l in self.discriminator_model.layers)\n",
    "        \n",
    "        # Train on a `batch_size` subset (selected with replacement) of the real images\n",
    "        batch_indices = rd.randint(0, x_train.shape[0], batch_size)\n",
    "        real_images_batch = x_train[batch_indices]\n",
    "        l, a = self.discriminator_model.train_on_batch(real_images_batch, np.ones((batch_size, 1)))\n",
    "        self.disc_loss_real.append(l)\n",
    "        self.disc_acc_real.append(a)\n",
    "        \n",
    "        # Train on a `batch_size` sample (in practice selected without replacement) of fake generated images\n",
    "        fake_image_low_dim = self._low_dim_image_sample(batch_size)\n",
    "        fake_images_batch = self.generator_model.predict(fake_image_low_dim)\n",
    "        l, a = self.discriminator_model.train_on_batch(fake_images_batch, np.zeros((batch_size, 1)))\n",
    "        self.disc_loss_fake.append(l)\n",
    "        self.disc_acc_fake.append(a)\n",
    "    \n",
    "    def _train_generator(self, batch_size: int) -> None:\n",
    "        \"\"\"\n",
    "        This _implicitly_ trains the generator, by training the entire generator-discriminator \n",
    "        model-pair. The problem is binary classification, where the input to the model is an\n",
    "        arbitrary low-dim representation, and the output is 1 (i.e. real). \n",
    "        \n",
    "        Since the discriminator should be frozen, the only trainable parameters are those in the\n",
    "        generator, so the only solution to the problem is to make the generator's images look\n",
    "        real, by updating the parameters of the generator.\n",
    "        \"\"\"\n",
    "        assert not self.discriminator_model.trainable, \\\n",
    "            \"Discriminator model should be frozen (i.e. not trainable) so we update only the generator\"\n",
    "        assert all(not l.trainable for l in self.discriminator_model.layers)\n",
    "        \n",
    "        fake_image_low_dim = self._low_dim_image_sample(batch_size)\n",
    "        l, a = self.compiled_model.train_on_batch(fake_image_low_dim, np.ones((batch_size, 1)))\n",
    "        self.gen_loss.append(l)\n",
    "        self.gen_acc.append(a)\n",
    "        \n",
    "    \n",
    "    def _build_discriminator(self) -> Model:\n",
    "        def n(part: str, idx: int) -> str:\n",
    "            return f\"disc_{part}_{idx:02}\"\n",
    "        \n",
    "        input_layer = Input(shape=self.input_dim, name=\"disc_input\")\n",
    "        x = input_layer\n",
    "        for idx, (fltrs, ksize, stride) in enumerate(zip(self.disc_conv_filter_count,\n",
    "                                                         self.disc_conv_kernel_size,\n",
    "                                                         self.disc_conv_strides)\n",
    "                                                    ):\n",
    "            idx += 1  # One indexed\n",
    "            x = Conv2D(filters=fltrs,\n",
    "                       kernel_size=ksize,\n",
    "                       strides=stride,\n",
    "                       padding=\"same\",\n",
    "                       name=n('conv', idx))(x)\n",
    "            if self.disc_batch_norm_momentum:\n",
    "                x = BatchNormalization(momentum=self.disc_batch_norm_momentum,\n",
    "                                       name=n('bnorm', idx))(x)\n",
    "            x = Activation(self.disc_activation,\n",
    "                           name=n(self.disc_activation, idx)\n",
    "                          )(x)\n",
    "            if self.disc_dropout_rate:\n",
    "                x = Dropout(self.disc_dropout_rate, name=n('dropout', idx))(x)\n",
    "            \n",
    "        x = Flatten(name='disc_flatten')(x)\n",
    "        result = Dense(units=1, # reinforcement style yes/no learning\n",
    "                       activation='sigmoid',\n",
    "                       kernel_initializer=self.weight_init,\n",
    "                       name='disc_output')(x)\n",
    "        \n",
    "        return Model(input_layer, result)\n",
    "    \n",
    "    def _build_generator(self) -> Model:\n",
    "        def n(part: str, idx: int) -> str:\n",
    "            return f\"gen_{part}_{idx:02}\"\n",
    "        \n",
    "        input_layer = Input(shape=self.z_dim, name='gen_input')\n",
    "        x = input_layer\n",
    "        \n",
    "        # Do a Dense product on the flattend representation, then reshape\n",
    "        # it so that it looks like \n",
    "        x = Dense(np.prod(self.gen_initial_dense_layer_size), name=n('expand', 0))(x)\n",
    "        if self.gen_batch_norm_momentum:\n",
    "            x = BatchNormalization(momentum=self.gen_batch_norm_momentum, name=n('bnorm', 0))(x)\n",
    "        x = Activation(self.gen_activation, name=n(self.gen_activation, 0))(x)\n",
    "        x = Reshape(target_shape=self.gen_initial_dense_layer_size, name=n('reshape', 0))(x)\n",
    "        if self.gen_dropout_rate:\n",
    "            x = Dropout(self.gen_dropout_rate, name=n('dropout', 0))(x)\n",
    "        \n",
    "        for idx, (ups, fltrs, ksize, stride) in enumerate(zip(self.gen_upsample,\n",
    "                                                              self.gen_conv_filter_count,\n",
    "                                                              self.gen_conv_kernel_size,\n",
    "                                                              self.gen_conv_strides\n",
    "                                                             )\n",
    "                                                          ):\n",
    "            idx += 1  # one-indexed\n",
    "\n",
    "            if ups != 1:\n",
    "                x = UpSampling2D(size=(ups, ups), name=n(f\"up_{ups}\", idx))(x)\n",
    "            x = Conv2D(filters=fltrs, \n",
    "                       kernel_size=ksize, \n",
    "                       strides=stride, \n",
    "                       padding='same',\n",
    "                       name=n('conv', idx))(x)\n",
    "            if self.gen_batch_norm_momentum:\n",
    "                x = BatchNormalization(momentum=self.gen_batch_norm_momentum, name=n('bnorm', idx))(x)\n",
    "            \n",
    "            act = 'tanh' if idx == len(self.gen_conv_filter_count) else self.gen_activation\n",
    "            x = Activation(act, name=n(self.gen_activation, idx))(x)\n",
    "#             if self.gen_dropout_rate:  # Apparently this is not in the generator, but it was in the Celebfaces decoder ?!\n",
    "#                 x = Dropout(self.gen_dropout_rate, name=n(\"dropout\", idx))\n",
    "            \n",
    "        result = x\n",
    "        return Model(input_layer, result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ace6428",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_builder = GanBuilder(\n",
    "    input_dim = QUICK_DRAW_SHAPE,\n",
    "    \n",
    "    disc_conv_filter_count=[64, 64, 128, 128],\n",
    "    disc_conv_kernel_size=[5, 5, 5, 5],\n",
    "    disc_conv_strides=[2, 2, 2, 1],\n",
    "    disc_batch_norm_momentum=None,\n",
    "    disc_activation='relu',  # Note harder node compared to VAE\n",
    "    disc_dropout_rate=0.4,\n",
    "    disc_learning_rate=0.0006,\n",
    "    \n",
    "    gen_initial_dense_layer_size=(7, 7, 64),\n",
    "    gen_upsample = [2, 2, 1, 1],\n",
    "    gen_conv_filter_count =[128, 64, 64, 1],\n",
    "    gen_conv_kernel_size=[5, 5, 5, 5],\n",
    "    gen_conv_strides=[1, 1, 1, 1],\n",
    "    gen_batch_norm_momentum=0.9,  # Note swap of dropout/bnorm vs disc\n",
    "    gen_activation='relu',\n",
    "    gen_dropout_rate=None,\n",
    "    gen_learning_rate=0.0002,\n",
    "    \n",
    "    optimiser='rms_prop',\n",
    "    weight_init=tf.keras.initializers.RandomNormal(mean=0., stddev=0.2),\n",
    "    z_dim=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920d426e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_builder.discriminator_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d714e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_builder.generator_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820c020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gan_builder.compiled_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0fab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d911d171",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_builder.discriminator_model.trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2dbacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d2541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_flow_shape = tuple([len(camels)] + list(QUICK_DRAW_SHAPE))\n",
    "camels_to_train = camels.reshape(tensor_flow_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6993df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_and_scale(arr: np.ndarray) -> Tuple[float, float, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Centre and scale the dataset.\n",
    "    \n",
    "    Return the mean and std before centering and scaling, and\n",
    "    the transformed dataset.\n",
    "    \"\"\"\n",
    "    mn = np.mean(arr)\n",
    "    sd = np.std(arr)\n",
    "    \n",
    "    return mn, sd, (arr - mn) / sd\n",
    "\n",
    "mn, sd, camels_to_train = center_and_scale(camels_to_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38929fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(camels_to_train[12132], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0df948",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gan_builder.train(x_train=camels_to_train, epoch_count=1000, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afb25a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(0, gan_builder.epochs_completed + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f518b0dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(epochs, gan_builder.disc_loss_real, label='D-Real')\n",
    "plt.plot(epochs, gan_builder.disc_loss_fake, label='D-Fake')\n",
    "plt.plot(epochs[:-1], gan_builder.gen_loss, label='G')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11396c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_images = gan_builder.generator_model.predict(\n",
    "    gan_builder._low_dim_image_sample(batch_size=64)\n",
    ")\n",
    "plot_camels(predicted_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5987b3a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3599174",
   "metadata": {},
   "source": [
    "# To release memory, in order to switch to a different notebook\n",
    "\n",
    "First check memory with \n",
    "\n",
    "```\n",
    "nvidia-smi\n",
    "```\n",
    "\n",
    "or, if you have installed it\n",
    "\n",
    "```\n",
    "gpustat\n",
    "```\n",
    "\n",
    "Then there are two approaches. The first uses Keras\n",
    "\n",
    "```\n",
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()\n",
    "```\n",
    "\n",
    "the second is\n",
    "\n",
    "```\n",
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()\n",
    "```\n",
    "\n",
    "I suspect if I were using a managed session, instead of an interactive one, I might not have to use these hacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac7d93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numba import cuda\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1d3f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59e808b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
