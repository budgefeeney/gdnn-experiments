{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_mQVCYrvLqd"
   },
   "source": [
    "# Abstract\n",
    "\n",
    "**Objective:** To build and run a convolutional neural network\n",
    "\n",
    "**Method:** We recreate the simple model from the first notebook in this series, and than an analoguous one, on a same dataset (CIFAR10). We then create a second model using convolutional layers\n",
    "\n",
    "**Observations & Results:** For convolution you need to be aware of strides (how many steps you take right and then down) and padding (how far out of the image you start and end). If `padding=same` and `strides=1` then the output will have the same height and width of the input (pad with zeros to accomplish this). If `padding=same` and `strides=2` then the output will be half the height & width. Without padding the output width will be the input - kernel_size, and similarly for height.\n",
    "\n",
    "Note that unlike a dense layer, which only consumes the last dimension, a convolutional layer will consume all-but-the-first dimension.\n",
    "\n",
    "Convolution with $A$ is just $||A X||_1^1$, i.e. `np.sum(A * X)`\n",
    "\n",
    "If your loss function starts returning `NaN` you may have an exploding graident problem, for which batch-normalization exists. Why this works needs some more explaining.\n",
    "\n",
    "With BatchNormalization and LeakyReLU, Dropout seems not to be as necessary as it once was to avoid overfitting.\n",
    "\n",
    "Conv models are massively slower to train than normal models despite the drastic reduction in parameter count. It seems max-pooling is no longer in vogue.\n",
    "\n",
    "**Conclusions:** Convolutional models are powerful, but slow-to-train, method of learning feature-representations. With LeakyReLUs and BatchNormalization, Dropout & pre-training are no longer as important. Further MaxPooling is less and less important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3-b3nB4vdUj"
   },
   "source": [
    "# Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4d9Eqtfivfrf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import keras.utils as kutils\n",
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rhF5COArz9G1"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "x_train = x_train.astype(np.float32) / 255\n",
    "x_test  = x_test.astype(np.float32) / 255\n",
    "\n",
    "y_train = kutils.to_categorical(y_train)\n",
    "y_test  = kutils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQjBjsfLvYeQ"
   },
   "source": [
    "# Simple Non-Convolutional Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IqB-yRjMvIed"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.models import Model \n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "input_layer = Input(shape=(32, 32, 3))\n",
    "\n",
    "x = Flatten()(input_layer)\n",
    "x = Dense(units=200, activation='relu')(x)\n",
    "x = Dense(units=150, activation='relu')(x)\n",
    "\n",
    "output_layer = Dense(units=10, activation='softmax')(x)\n",
    "\n",
    "nn = Model(input_layer, output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hZf3k9HP0nLU"
   },
   "outputs": [],
   "source": [
    "opt = Adam(learning_rate=0.005)\n",
    "\n",
    "nn.compile(optimizer=opt,\n",
    "           loss='categorical_crossentropy',\n",
    "           metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 778,
     "status": "ok",
     "timestamp": 1609436546591,
     "user": {
      "displayName": "Bryan Feeney",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiPntWv82gXOOwC1JBFNoK4SPM-4IxTQnERyCBuUQ=s64",
      "userId": "15000880758463632242"
     },
     "user_tz": 0
    },
    "id": "kbWODPTa5LjX",
    "outputId": "c4d75179-bcac-48e3-a196-536eb0bc4c5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 200)               614600    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 150)               30150     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                1510      \n",
      "=================================================================\n",
      "Total params: 646,260\n",
      "Trainable params: 646,260\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 136838,
     "status": "ok",
     "timestamp": 1609436189991,
     "user": {
      "displayName": "Bryan Feeney",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiPntWv82gXOOwC1JBFNoK4SPM-4IxTQnERyCBuUQ=s64",
      "userId": "15000880758463632242"
     },
     "user_tz": 0
    },
    "id": "b83kLZYA06GZ",
    "outputId": "68e464b7-3590-4e4d-924a-977a21edcc44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 2.2970 - accuracy: 0.2498 - val_loss: 1.8078 - val_accuracy: 0.3402\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.8429 - accuracy: 0.3314 - val_loss: 1.8650 - val_accuracy: 0.3243\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.8145 - accuracy: 0.3378 - val_loss: 1.8524 - val_accuracy: 0.3309\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.8076 - accuracy: 0.3387 - val_loss: 1.8251 - val_accuracy: 0.3285\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.7828 - accuracy: 0.3537 - val_loss: 1.8160 - val_accuracy: 0.3340\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.7852 - accuracy: 0.3470 - val_loss: 1.8339 - val_accuracy: 0.3258\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.7823 - accuracy: 0.3496 - val_loss: 1.7537 - val_accuracy: 0.3524\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.7671 - accuracy: 0.3540 - val_loss: 1.7773 - val_accuracy: 0.3412\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 1.7639 - accuracy: 0.3565 - val_loss: 1.7680 - val_accuracy: 0.3598\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.7692 - accuracy: 0.3514 - val_loss: 1.7533 - val_accuracy: 0.3614\n",
      "Epoch 1/5\n",
      "196/196 [==============================] - 5s 24ms/step - loss: 1.6949 - accuracy: 0.3817 - val_loss: 1.6979 - val_accuracy: 0.3794\n",
      "Epoch 2/5\n",
      "196/196 [==============================] - 4s 21ms/step - loss: 1.6850 - accuracy: 0.3844 - val_loss: 1.6955 - val_accuracy: 0.3813\n",
      "Epoch 3/5\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 1.6844 - accuracy: 0.3848 - val_loss: 1.6972 - val_accuracy: 0.3813\n",
      "Epoch 4/5\n",
      "196/196 [==============================] - 4s 21ms/step - loss: 1.6801 - accuracy: 0.3864 - val_loss: 1.6975 - val_accuracy: 0.3824\n",
      "Epoch 5/5\n",
      "196/196 [==============================] - 4s 22ms/step - loss: 1.6809 - accuracy: 0.3874 - val_loss: 1.6901 - val_accuracy: 0.3840\n",
      "Epoch 1/5\n",
      "25/25 [==============================] - 3s 132ms/step - loss: 1.6692 - accuracy: 0.3903 - val_loss: 1.6827 - val_accuracy: 0.3845\n",
      "Epoch 2/5\n",
      "25/25 [==============================] - 3s 132ms/step - loss: 1.6603 - accuracy: 0.3929 - val_loss: 1.6811 - val_accuracy: 0.3845\n",
      "Epoch 3/5\n",
      "25/25 [==============================] - 3s 127ms/step - loss: 1.6601 - accuracy: 0.3944 - val_loss: 1.6820 - val_accuracy: 0.3839\n",
      "Epoch 4/5\n",
      "25/25 [==============================] - 3s 128ms/step - loss: 1.6597 - accuracy: 0.3945 - val_loss: 1.6781 - val_accuracy: 0.3868\n",
      "Epoch 5/5\n",
      "25/25 [==============================] - 3s 127ms/step - loss: 1.6589 - accuracy: 0.3946 - val_loss: 1.6818 - val_accuracy: 0.3876\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f772b9d4080>"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit(x_train, y_train,\n",
    "          validation_data=(x_test, y_test),\n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "          shuffle=True)\n",
    "\n",
    "nn.fit(x_train, y_train,\n",
    "          validation_data=(x_test, y_test),\n",
    "          batch_size=256,\n",
    "          epochs=5,\n",
    "          shuffle=True)\n",
    "\n",
    "nn.fit(x_train, y_train,\n",
    "          validation_data=(x_test, y_test),\n",
    "          batch_size=2048,\n",
    "          epochs=5,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8_yHt6w3WC2"
   },
   "source": [
    "# Convolution Model Take #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9j8cD76x3VbB"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Flatten, Dense, Conv2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eVGfA6uD1L5A"
   },
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(32, 32, 3))\n",
    "\n",
    "conv_1 = Conv2D(\n",
    "  filters = 10,  # For every channel we create 10 projections\n",
    "  kernel_size=(4, 4),\n",
    "  strides=2,\n",
    "  padding='same',   \n",
    ")(input_layer)\n",
    "\n",
    "conv_2 = Conv2D(\n",
    "    filters=20, # for every projection (channel-count x 10) we create 20 projections\n",
    "    kernel_size=(3,3),\n",
    "    strides=2,\n",
    "    padding='same'\n",
    ")(conv_1)\n",
    "\n",
    "flatten_3 = Flatten()(conv_2)\n",
    "\n",
    "output_layer = Dense(units=10, activation='softmax')(flatten_3)\n",
    "\n",
    "dnn = Model(input_layer, output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 506,
     "status": "ok",
     "timestamp": 1609436565366,
     "user": {
      "displayName": "Bryan Feeney",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiPntWv82gXOOwC1JBFNoK4SPM-4IxTQnERyCBuUQ=s64",
      "userId": "15000880758463632242"
     },
     "user_tz": 0
    },
    "id": "3iNTer0y5J-9",
    "outputId": "438b3e5e-6f52-401a-edf6-6e8b448855d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 16, 16, 10)        490       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 8, 8, 20)          1820      \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 10)                12810     \n",
      "=================================================================\n",
      "Total params: 15,120\n",
      "Trainable params: 15,120\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJ6CMHHZ5ZAY"
   },
   "source": [
    "First thing that leaps out is that there are radically fewer parameters in this model. \n",
    "\n",
    "The next step is explaining the evolution of the data as it passes through the network. We have\n",
    "\n",
    "```\n",
    "N x 32 x 32 x 3\n",
    "\n",
    "      |\n",
    "     \\|/\n",
    "      '\n",
    "N x16 x 16 x 10  (strides=2, padding=same)\n",
    "\n",
    "      |\n",
    "     \\|/\n",
    "      '\n",
    "N x 8 x 8 x 20 (strides=2, padding=same)\n",
    "\n",
    "      |\n",
    "     \\|/\n",
    "      '\n",
    "N x 1280   (64 x 20 = 1280)\n",
    "\n",
    "      |\n",
    "     \\|/\n",
    "      '\n",
    "N x 10   (Dense 1280 x 10 + 10)\n",
    "```\n",
    "\n",
    "So the first convolution transitions us from an 32 x 32 image with 3 channels, to a 16 x 16 image with 10 channels.\n",
    "\n",
    "There are 490 parameters. Each filter is 49 params (4 x 4 x 3 + 1), i.e. each filter consumes all three channels in one go! And also has an intercept term. The we have 10 filters to make 490 parameters total.\n",
    "\n",
    "So note that unlike a dense layer, which only consumes the last dimension, a convolutional layer will consume all-but-the-first dimension.\n",
    "\n",
    "FIXME Firm up the maths on this one.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weeYigIS75V0"
   },
   "source": [
    "## Convolution Model Take #2\n",
    "\n",
    "This will introduce BAD: BatchNormalization, Activation, Dropout\n",
    "\n",
    "If weights near the start of the net never change, you have a _vanishing gradient problem_.\n",
    "\n",
    "If hidden layer values never change, i.e. the unit has \"died\", it's gradient is always 0. So replace RELU with leaky relu to never have a zero gradient.\n",
    "\n",
    "If weights explode then you have the _exploding gradient problem_. This is usually indicated by your loss function returning `NaN`.\n",
    "\n",
    "Since weights are randomized to 0..1, we rescale input to -1..+1 in order to avoid massive gradients early on.\n",
    "\n",
    "However as the network is trained you may experience _covariate shift_. Back propagation assumes (informally) that the distribution of the input to a layer doesn't change. However it may change significantly, leading to cascading overcorrections, exploding the gradient.\n",
    "\n",
    "Batch normalization just centres and scales the output of one hidden layer before presenting it to the next. It has a sense of inertia (aka moementum) to ensure it doesn't over-correct data however, so the input isn't perfectly centred and scaled.\n",
    "\n",
    "Data are localled scaled and shifted: i.e. we center and scale as normal using population mean. Then we rescale and relocate that using global parameters $\\gamma$ (the global scale) and $\\beta$ the global location. These are updated with a momentum param.\n",
    "\n",
    "FIXME clarify maths on this\n",
    "\n",
    "BatchNormalization & RELU is usually okay on its own to prevent overfitting, so the old solution dropout is nowadays skipped entirely sometimes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISigTAh_74xz"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Flatten, Dense, Conv2D, BatchNormalization, \\\n",
    "  Dropout, LeakyReLU, Activation\n",
    "\n",
    "# This architecture has so many odds and ends it seems hard to believe it's\n",
    "# not been exhaustively tuned...\n",
    "input_layer = Input(shape=(32, 32, 3))\n",
    "\n",
    "x = Conv2D(\n",
    "    filters=32,\n",
    "    kernel_size=(3,3),\n",
    "    strides=1,\n",
    "    padding='same'\n",
    ")(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)  # Note how we need separate activations to insert BatchNorm\n",
    "\n",
    "x = Conv2D(\n",
    "    filters=32,\n",
    "    kernel_size=(3,3),\n",
    "    strides=2,\n",
    "    padding='same'\n",
    ")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Conv2D(\n",
    "    filters=64,\n",
    "    kernel_size=(3,3),\n",
    "    strides=1,\n",
    "    padding='same'\n",
    ")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "\n",
    "x = Conv2D(\n",
    "    filters=64,\n",
    "    kernel_size=(3,3),\n",
    "    strides=2,\n",
    "    padding='same'\n",
    ")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(128)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = Dropout(rate=0.5)(x)\n",
    "\n",
    "x = Dense(10)(x)\n",
    "output_layer = Activation('softmax')(x)\n",
    "\n",
    "dnn = Model(input_layer, output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 460,
     "status": "ok",
     "timestamp": 1609438291245,
     "user": {
      "displayName": "Bryan Feeney",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiPntWv82gXOOwC1JBFNoK4SPM-4IxTQnERyCBuUQ=s64",
      "userId": "15000880758463632242"
     },
     "user_tz": 0
    },
    "id": "TLovIVQU5Q1L",
    "outputId": "de585f2c-5ab7-4488-acfc-a4d5a6e295c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 128)               524416    \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 592,554\n",
      "Trainable params: 591,914\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9JtkE6Q-lgW"
   },
   "outputs": [],
   "source": [
    "opt = Adam(learning_rate=0.0005)\n",
    "dnn.compile(optimizer=opt,\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3189320,
     "status": "ok",
     "timestamp": 1609441622184,
     "user": {
      "displayName": "Bryan Feeney",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiPntWv82gXOOwC1JBFNoK4SPM-4IxTQnERyCBuUQ=s64",
      "userId": "15000880758463632242"
     },
     "user_tz": 0
    },
    "id": "zrzTQ9RtABkc",
    "outputId": "66dfc72f-23a4-487f-cfe0-6306dc333502"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 172s 110ms/step - loss: 1.8585 - accuracy: 0.3681 - val_loss: 1.2274 - val_accuracy: 0.5648\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 174s 111ms/step - loss: 1.2161 - accuracy: 0.5673 - val_loss: 1.0457 - val_accuracy: 0.6350\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 168s 108ms/step - loss: 1.0254 - accuracy: 0.6420 - val_loss: 0.9132 - val_accuracy: 0.6802\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 167s 107ms/step - loss: 0.9161 - accuracy: 0.6772 - val_loss: 0.8838 - val_accuracy: 0.6931\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 166s 106ms/step - loss: 0.8645 - accuracy: 0.6978 - val_loss: 0.8848 - val_accuracy: 0.6878\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 172s 110ms/step - loss: 0.7871 - accuracy: 0.7298 - val_loss: 0.8265 - val_accuracy: 0.7123\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 169s 108ms/step - loss: 0.7399 - accuracy: 0.7429 - val_loss: 0.8226 - val_accuracy: 0.7130\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 165s 105ms/step - loss: 0.7059 - accuracy: 0.7533 - val_loss: 1.1232 - val_accuracy: 0.6341\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 166s 106ms/step - loss: 0.6639 - accuracy: 0.7651 - val_loss: 0.7732 - val_accuracy: 0.7326\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 168s 108ms/step - loss: 0.6399 - accuracy: 0.7780 - val_loss: 0.9244 - val_accuracy: 0.6931\n",
      "Epoch 1/5\n",
      "196/196 [==============================] - 150s 764ms/step - loss: 0.4932 - accuracy: 0.8335 - val_loss: 0.6983 - val_accuracy: 0.7654\n",
      "Epoch 2/5\n",
      "196/196 [==============================] - 147s 751ms/step - loss: 0.4555 - accuracy: 0.8461 - val_loss: 0.7035 - val_accuracy: 0.7625\n",
      "Epoch 3/5\n",
      "196/196 [==============================] - 147s 748ms/step - loss: 0.4322 - accuracy: 0.8531 - val_loss: 0.7120 - val_accuracy: 0.7593\n",
      "Epoch 4/5\n",
      "196/196 [==============================] - 149s 759ms/step - loss: 0.4170 - accuracy: 0.8579 - val_loss: 0.7217 - val_accuracy: 0.7550\n",
      "Epoch 5/5\n",
      "196/196 [==============================] - 149s 760ms/step - loss: 0.4039 - accuracy: 0.8614 - val_loss: 0.7414 - val_accuracy: 0.7567\n",
      "Epoch 1/5\n",
      "49/49 [==============================] - 152s 3s/step - loss: 0.3665 - accuracy: 0.8767 - val_loss: 0.7208 - val_accuracy: 0.7634\n",
      "Epoch 2/5\n",
      "49/49 [==============================] - 153s 3s/step - loss: 0.3590 - accuracy: 0.8785 - val_loss: 0.7288 - val_accuracy: 0.7615\n",
      "Epoch 3/5\n",
      "49/49 [==============================] - 152s 3s/step - loss: 0.3554 - accuracy: 0.8796 - val_loss: 0.7290 - val_accuracy: 0.7629\n",
      "Epoch 4/5\n",
      "49/49 [==============================] - 153s 3s/step - loss: 0.3470 - accuracy: 0.8818 - val_loss: 0.7489 - val_accuracy: 0.7545\n",
      "Epoch 5/5\n",
      "49/49 [==============================] - 150s 3s/step - loss: 0.3383 - accuracy: 0.8852 - val_loss: 0.7326 - val_accuracy: 0.7624\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f772b05d240>"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn.fit(x_train, y_train,\n",
    "        validation_data=(x_test, y_test),\n",
    "        batch_size=32,\n",
    "        epochs=10,\n",
    "        shuffle=True)\n",
    "\n",
    "dnn.fit(x_train, y_train,\n",
    "        validation_data=(x_test, y_test),\n",
    "        batch_size=256,\n",
    "        epochs=5,\n",
    "        shuffle=True)\n",
    "\n",
    "dnn.fit(x_train, y_train,\n",
    "        validation_data=(x_test, y_test),\n",
    "        batch_size=1024,\n",
    "        epochs=5,\n",
    "        shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYehuyGeCDfU"
   },
   "source": [
    "It must be said, given the reduction in parameters, that this comparisoon of methods seems to be a little bit of a cheat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Q675MIRAUmf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP+9nSVMfHi0uh6dLlwep+4",
   "collapsed_sections": [],
   "name": "Deep Learning 1.2 - Basic Convolutional Network.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
